import numpy as np
from keras import ops

from keras_hub.src.api_export import keras_hub_export
from keras_hub.src.models.image_segmenter import ImageSegmenter
from keras_hub.src.models.sam2.sam2_backbone import SAM2Backbone
from keras_hub.src.models.sam2.sam2_image_segmenter_preprocessor import (
    SAM2ImageSegmenterPreprocessor,
)


@keras_hub_export("keras_hub.models.SAM2ImageSegmenter")
class SAM2ImageSegmenter(ImageSegmenter):
    """The Segment Anything Model 2 (SAM2) Image Segmenter.

    SAM2 works by prompting the input images. There are three ways to prompt:
    (1) Labelled Points: Foreground points (points with label 1) are encoded
        such that the output masks generated by the mask decoder contain them
        and background points (points with label 0) are encoded such that the
        generated masks don't contain them.
    (2) Box: A box tells the model which part/crop of the image to segment.
    (3) Mask: An input mask can be used to refine the output of the mask
        decoder.
    These prompts can be mixed and matched but at least one of the prompts
    must be present. To turn off a particular prompt, simply exclude it from
    the inputs to the model.
    (1) For points prompts, the expected shape is `(batch, num_points, 2)`.
        The labels must have a corresponding shape of `(batch, num_points)`.
    (2) For box prompt, the expected shape is `(batch, 1, 2, 2)`.
    (3) Similarly, mask prompts have shape `(batch, 1, H, W, 1)`.

    Args:
        backbone: A `keras_hub.models.SAM2Backbone` instance.
        preprocessor: A `keras_hub.models.SAM2ImageSegmenterPreprocessor` or
            `None`. If `None`, this model will not apply preprocessing to the
            inputs.

    Example:
    Load pretrained model using `from_preset`.

    ```python
    # Note: Presets not yet available on Kaggle Models, placeholder example.
    # sam = keras_hub.models.SAM2ImageSegmenter.from_preset('sam2_hiera_tiny')
    ```

    Load segment anything model 2 with custom backbone

    ```python
    image_size = 1024
    batch_size = 2
    images = np.ones(
        (batch_size, image_size, image_size, 3),
        dtype="float32",
    )

    image_encoder = keras_hub.models.SAM2ImageEncoder(
        embed_dim=96,
        num_heads=1,
        stages=(1, 2, 7, 2),
        global_att_blocks=(5, 7, 9),
        window_pos_embed_bkg_spatial_size=(7, 7),
        window_spec=(8, 4, 14, 7),
        d_model=256,
        backbone_channel_list=[96, 192, 384, 768],
        scalp=1
    )
    memory_attention = keras_hub.layers.SAM2MemoryAttention(
        d_model=256,
        num_layers=4,
        num_heads=1,
        rope_feat_sizes=(64, 64)
    )
    memory_encoder = keras_hub.layers.SAM2MemoryEncoder(
        out_dim=64,
        in_dim=256,
        fuser_num_layers=2
    )
    prompt_encoder = keras_hub.models.SAMPromptEncoder(
        hidden_size=256,
        image_embedding_size=(64, 64),
        input_image_size=(1024, 1024),
        mask_in_channels=16
    )
    mask_decoder = keras_hub.layers.SAM2MaskDecoder(
        hidden_size=256,
        num_layers=2,
        intermediate_dim=2048,
        num_heads=8,
        embedding_dim=256,
        num_multimask_outputs=3,
        pred_obj_scores=True
    )

    backbone = keras_hub.models.SAM2Backbone(
        image_encoder=image_encoder,
        memory_attention=memory_attention,
        memory_encoder=memory_encoder,
        prompt_encoder=prompt_encoder,
        mask_decoder=mask_decoder,
    )
    sam = keras_hub.models.SAM2ImageSegmenter(
        backbone=backbone
    )

    # Predict
    points = np.array([[[512., 512.], [100., 100.]]])
    labels = np.array([[1., 0.]])
    inputs = {
        "images": images,
        "points": points,
        "labels": labels,
    }
    outputs = sam.predict(inputs)
    masks, iou_pred = outputs["masks"], outputs["iou_pred"]
    ```
    """

    backbone_cls = SAM2Backbone
    preprocessor_cls = SAM2ImageSegmenterPreprocessor

    def __init__(self, backbone, preprocessor=None, **kwargs):
        # === Functional Model ===
        inputs = backbone.input
        backbone_outputs = backbone(inputs)

        decoder_inputs = {
            "image_embeddings": backbone_outputs["vision_features"],
            "image_pe": backbone_outputs["vision_pos_enc"],
            "sparse_prompt_embeddings": backbone_outputs[
                "prompt_sparse_embeddings"
            ],
            "dense_prompt_embeddings": backbone_outputs[
                "prompt_dense_embeddings"
            ],
            "multimask_output": True,
        }

        outputs = backbone.mask_decoder(**decoder_inputs)

        super().__init__(
            inputs=inputs,
            outputs=outputs,
            backbone=backbone,
            preprocessor=preprocessor,
            **kwargs,
        )

    def predict_step(self, *args, **kwargs):
        if len(args) == 2:
            args = (args[0], self._add_placeholder_prompts(args[-1]))
        else:
            args = (self._add_placeholder_prompts(args[0]),)

        return super().predict_step(*args, **kwargs)

    def _add_placeholder_prompts(self, inputs):
        inputs = inputs.copy()
        batch_size = ops.shape(inputs["images"])[0]
        is_tensor = ops.is_tensor(inputs["images"])
        zeros = ops.zeros if is_tensor else np.zeros

        if "points" not in inputs:
            inputs["points"] = zeros((batch_size, 0, 2), dtype="float32")
        if "labels" not in inputs:
            inputs["labels"] = zeros((batch_size, 0), dtype="int32")
        if "boxes" not in inputs:
            inputs["boxes"] = zeros((batch_size, 0, 2, 2), dtype="float32")

        return inputs
